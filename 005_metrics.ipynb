{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "y_true = [0, 1, 0, 0, 1, 0]\n",
    "y_pred = [0, 1, 1, 0, 1, 1]\n",
    "```\n",
    "$$\n",
    "TPR=\\frac{2}{2}=1 \\\\\n",
    "FPR=\\frac{2}{4}=0.5\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive Rate (TPR) or Recall or Sensitivity**\n",
    "$$\n",
    "TPR=\\frac{TP}{P}=\\frac{TP}{TP+FN}\n",
    "$$\n",
    "**False Positive Rate (FPR)**\n",
    "$$\n",
    "FPR=\\frac{FP}{N}=\\frac{FP}{FP+TN}\n",
    "$$\n",
    "**Precision**\n",
    "$$\n",
    "Precision=\\frac{TP}{TP+FP}\n",
    "$$\n",
    "**Specificity**\n",
    "$$\n",
    "Specificity=\\frac{TN}{TN+FP}\n",
    "$$\n",
    "**F1 score**\n",
    "$$\n",
    "F_1 = \\frac{2\\times TP}{2\\times TP + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, \\\n",
    "    roc_curve, accuracy_score , f1_score, \\\n",
    "    precision_score, recall_score, confusion_matrix, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    \"\"\"Returns a dictionary of metrics for a given true and predicted set of labels.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred)\n",
    "    metrics['recall'] = recall_score(y_true, y_pred)\n",
    "    metrics['f1'] = f1_score(y_true, y_pred)\n",
    "    metrics['specificity'] = tn / (tn + fp)\n",
    "    metrics['fpr'] = fp / (fp + tn)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_curves(y_true, y_score):\n",
    "    \"\"\"Returns a dictionary of metrics for a given true and predicted set of labels.\n",
    "    \"\"\"\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_score)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return {'precision': precision, 'recall': recall, 'pr_thresholds': pr_thresholds,\n",
    "            'fpr': fpr, 'tpr': tpr, 'roc_thresholds': roc_thresholds,\n",
    "            'pr_auc': pr_auc, 'roc_auc': roc_auc}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f67cd72fb8bdd850b488b361a9e03c178ee3dd3d3ad96eafda1508542fce4e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
